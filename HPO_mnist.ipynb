{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras MNIST classifier with random-search hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# System imports\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "# Data libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep learning\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Local imports\n",
    "from mnist import load_data, build_model\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrency configuration\n",
    "n_inter_threads = 2\n",
    "n_intra_threads = 6\n",
    "config = tf.ConfigProto(\n",
    "    inter_op_parallelism_threads=n_inter_threads,\n",
    "    intra_op_parallelism_threads=n_intra_threads,\n",
    ")\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "y_train shape: (60000, 10)\n",
      "y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Data config\n",
    "n_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "# Read the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reformat and scale the data\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.reshape(x_train.shape[0], *input_shape).astype(np.float32) / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], *input_shape).astype(np.float32) / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model and training procedure"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def build_model(h1=4, h2=8, h3=32, dropout=0.5,\n",
    "                optimizer_type=keras.optimizers.Adadelta):\n",
    "    \"\"\"Construct our Keras model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(h1, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(h2, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(h3, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    opt = optimizer_type()\n",
    "    model.compile(optimizer=opt, loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily making things reproducible for development\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyper-parameter search points\n",
    "n_hpo_trials = 16\n",
    "\n",
    "grid_h1 = np.random.choice([4, 8, 16, 32, 64], size=n_hpo_trials)\n",
    "grid_h2 = np.random.choice([4, 8, 16, 32, 64], size=n_hpo_trials)\n",
    "grid_h3 = np.random.choice([8, 16, 32, 64, 128], size=n_hpo_trials)\n",
    "grid_dropout = np.random.rand(n_hpo_trials)\n",
    "grid_optimizer = np.random.choice(\n",
    "    [keras.optimizers.Adadelta, keras.optimizers.Adam, keras.optimizers.Nadam],\n",
    "    size=n_hpo_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "batch_size = 128\n",
    "n_epochs = 16\n",
    "valid_frac = 0.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(x_train, y_train, valid_frac, batch_size, n_epochs,\n",
    "                    h1, h2, h3, dropout, optimizer, verbose=0):\n",
    "    # Build the model\n",
    "    model = build_model(h1=h1, h2=h2, h3=h3,\n",
    "                        dropout=dropout,\n",
    "                        optimizer_type=optimizer)\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_split=valid_frac,\n",
    "                        batch_size=batch_size, epochs=n_epochs,\n",
    "                        verbose=verbose)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter trial 0\n",
      "  Hidden 64, 8, 16\n",
      "  Dropout 0.3865\n",
      "  Optimizer Adadelta\n",
      "  Final score 0.979607843231\n",
      "Hyper-parameter trial 1\n",
      "  Hidden 4, 4, 16\n",
      "  Dropout 0.9026\n",
      "  Optimizer Adadelta\n",
      "  Final score 0.685588235481\n",
      "Hyper-parameter trial 2\n",
      "  Hidden 32, 8, 16\n",
      "  Dropout 0.4499\n",
      "  Optimizer Adadelta\n",
      "  Final score 0.973137254995\n",
      "Hyper-parameter trial 3\n",
      "  Hidden 32, 64, 8\n",
      "  Dropout 0.6131\n",
      "  Optimizer Adadelta\n"
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "all_histories = []\n",
    "\n",
    "# Loop over hps\n",
    "for ihp in range(n_hpo_trials):\n",
    "    print('Hyper-parameter trial', ihp)\n",
    "    print('  Hidden %i, %i, %i' % (grid_h1[ihp], grid_h2[ihp], grid_h3[ihp]))\n",
    "    print('  Dropout %.4f' % grid_dropout[ihp])\n",
    "    print('  Optimizer', grid_optimizer[ihp].__name__)\n",
    "    \n",
    "    model, history = build_and_train(x_train, y_train, valid_frac=valid_frac,\n",
    "                                     batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                     h1=grid_h1[ihp], h2=grid_h2[ihp], h3=grid_h3[ihp],\n",
    "                                     dropout=grid_dropout[ihp],\n",
    "                                     optimizer=grid_optimizer[ihp])\n",
    "    \n",
    "    # Use final validation accuracy as objective metric\n",
    "    score = history.history['val_acc'][-1]\n",
    "    print('  Final score', score)\n",
    "    \n",
    "    # Save the model and history\n",
    "    all_models.append(model)\n",
    "    all_histories.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_last_scores = np.array([h.history['val_acc'][-1] for h in all_histories])\n",
    "all_best_scores = np.array([max(h.history['val_acc']) for h in all_histories])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best in terms of final validation set accuracy\n",
    "i = all_last_scores.argmax()\n",
    "h = all_histories[i]\n",
    "\n",
    "print('Hyperparameters: trial %i hidden %i-%i-%i dropout %.3f opt %s' %\n",
    "      (i, grid_h1[i], grid_h2[i], grid_h3[i], grid_dropout[i], grid_optimizer[i].__name__))\n",
    "print('  Last validation accuracy %.4f' % all_last_scores[i])\n",
    "print('  Best validation accuracy %.4f' % all_best_scores[i])\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(121)\n",
    "plt.plot(h.epoch, h.history['loss'], label='Training')\n",
    "plt.plot(h.epoch, h.history['val_loss'], label='Validation')\n",
    "plt.xlim((min(h.epoch), max(h.epoch)))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(122)\n",
    "plt.plot(h.history['acc'], label='Training')\n",
    "plt.plot(h.history['val_acc'], label='Validation')\n",
    "plt.xlim((min(h.epoch), max(h.epoch)))\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the worst in terms of final validation set accuracy\n",
    "i = all_last_scores.argmin()\n",
    "h = all_histories[i]\n",
    "\n",
    "print('Hyperparameters: trial %i hidden %i-%i-%i dropout %.3f opt %s' %\n",
    "      (i, grid_h1[i], grid_h2[i], grid_h3[i], grid_dropout[i], grid_optimizer[i].__name__))\n",
    "print('  Last validation accuracy %.4f' % all_last_scores[i])\n",
    "print('  Best validation accuracy %.4f' % all_best_scores[i])\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(121)\n",
    "plt.plot(h.epoch, h.history['loss'], label='Training')\n",
    "plt.plot(h.epoch, h.history['val_loss'], label='Validation')\n",
    "plt.xlim((min(h.epoch), max(h.epoch)))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(122)\n",
    "plt.plot(h.history['acc'], label='Training')\n",
    "plt.plot(h.history['val_acc'], label='Validation')\n",
    "plt.xlim((min(h.epoch), max(h.epoch)))\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = all_models[all_last_scores.argmax()]\n",
    "score = best_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "debug180313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
